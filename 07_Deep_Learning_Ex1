# Neural network (logistic 3개 이용) 을 이용하여 XOR 연산을 학습

import tensorflow as tf
# training data set
x_data = [[0,0],[0,1],[1,0],[1,1]]
y_data = [[0],[1],[1],[0]]

#  placeholder 
X = tf.placeholder(shape=[None,2], dtype = tf.float32)
Y = tf.placeholder(shape=[None,1], dtype = tf.float32)

# weight & bias
# W1의 [행,열] 중 열 값 == 다음 layer의 입력값 갯수
# b1의 [갯수] == 다음 layer의 입력값 갯수
W1 = tf.Variable(tf.random_normal([2,8]), name = "weight1")
b1 = tf.Variable(tf.random_normal([8]), name = "bias1")

# 다음 layer의 입력값
layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)

W2 = tf.Variable(tf.random_normal([8,1]), name = "weight2")
b2 = tf.Variable(tf.random_normal([1]), name = "bias2")

# hypothesis
logits = tf.matmul(layer1,W2) +b2
H =  tf.sigmoid(logits)

# cost function
cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels = Y))

# train node 생성
optimizer = tf.train.GradientDescentOptimizer(learning_rate= 0.01)
train = optimizer.minimize(cost)

# session & 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습
for step in range(3000):
    _, cost_val = sess.run([train,cost], feed_dict = {X:x_data , Y:y_data})
    if step % 300 == 0:
        print(cost_val)

# accuracy 측정
predict = tf.cast(H> 0.5 , dtype = tf.float32)
correct = tf.equal(predict, Y)
accuracy = tf.reduce_mean(tf.cast(correct,dtype = tf.float32))

print("정확도 : {}".format(sess.run(accuracy , feed_dict = {X:x_data,
                                         Y:y_data})))
print("H : {}".format(sess.run(H,feed_dict = {X:[[1,1]]})))
