{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class\n",
    "# 1. 객체 모델링의 수단\n",
    "# 2. ADT (Abstract data type) - 추상 데이터 타입\n",
    "\n",
    "# class안에 정의되는 내용은 크게 3가지\n",
    "# 1. 상태값 저장을 위한 변수(field, member variable, property)\n",
    "# 2. 수행하는 작업을 위한 함수(method, member function, method)\n",
    "# 3. 정의된 class 정보를 바탕으로 일정 메모리를 확보\n",
    "#    => 확보된 메모리 공간 -> 인스턴스, 객체\n",
    "#    이런 객체를 생성하기 위해 생성자가 호출되어야 함\n",
    "\n",
    "# 학생이 3명 있어요\n",
    "# 각 학생이 가지는 정보는 이름,국어,영어,수학 점수\n",
    "# 평균, 총점 결과를 알고 싶음\n",
    "\n",
    "class Student:\n",
    "    # constructor\n",
    "    # def __init__(self,매개변수 list)\n",
    "    def __init__(self,s_name,s_kor,s_eng,s_math):\n",
    "        # property를 생성하려면 self를 이용\n",
    "        # self를 붙이지 않으면 지역변수로 사용 -> 생성자 호출 후 사라짐 \n",
    "        self.student_name = s_name\n",
    "        self.kor = s_kor\n",
    "        self.eng = s_eng\n",
    "        self.math = s_math\n",
    "        self.get_total()\n",
    "    # method\n",
    "    def get_total(self):\n",
    "        self.get_total1()\n",
    "#         # self를 붙여야 property 지칭\n",
    "#         self.total = self.kor+self.eng+self.math   \n",
    "#         return self.total\n",
    "    def get_total1(self):\n",
    "        print(self.eng)\n",
    "stu1 = Student(\"홍길동\",10,20,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "학습\n",
      "0.06212642\n",
      "0.031970955\n",
      "0.021069486\n",
      "0.032512758\n",
      "0.008929146\n",
      "0.0015808411\n",
      "0.01472627\n",
      "0.09799273\n",
      "0.009671728\n",
      "0.00010744656\n",
      "0.040287446\n",
      "0.0010463244\n",
      "0.00015026034\n",
      "0.025938349\n",
      "0.0005252184\n"
     ]
    }
   ],
   "source": [
    "## Ensemble을 이용한 MNIST\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "## Graph 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Class Define\n",
    "## cnn 모델을 생성하는 class\n",
    "class CnnModel:\n",
    "        # constructor\n",
    "        def __init__(self,sess,name,m,test):\n",
    "            self.sess = sess\n",
    "            self.name = name\n",
    "            self.mnist = m\n",
    "            self.test_x_data = test\n",
    "            self.build_net()\n",
    "         \n",
    "        # tensorflow model graph(node) 생성하는 method    \n",
    "        def build_net(self):    \n",
    "            with tf.variable_scope(self.name):\n",
    "    #             mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "    #             mnist = pd.read_csv(\"./data/digitrecognizer/train.csv\")\n",
    "    #             num_of_train = int(mnist.shape[0]*0.7)\n",
    "    #             train_data = mnist.loc[:num_of_train,:]\n",
    "    #             test_data = mnist.loc[num_of_train+1:,:]\n",
    "    #             train_x_data = train_data.drop(\"label\",axis=1,inplace=False)\n",
    "    #             train_y_data = tf.one_hot(train_data[\"label\"], depth=10).eval(session = sess)\n",
    "    #             test_x_data = test_data.drop(\"label\",axis=1,inplace=False)\n",
    "    #             test_x_data = pd.read_csv(\"./data/digitrecognizer/test.csv\")\n",
    "\n",
    "                # tensorflow graph 초기화\n",
    "                self.X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "                self.Y = tf.placeholder(shape = [None,10], dtype = tf.float32)\n",
    "                self.drop_rate = tf.placeholder(dtype = tf.float32)\n",
    "                X_img = tf.reshape(self.X,[-1,28,28,1])\n",
    "\n",
    "                L1 = tf.layers.conv2d(inputs=X_img,\n",
    "                                      filters = 32,\n",
    "                                      kernel_size=[3,3],\n",
    "                                      padding= \"SAME\",\n",
    "                                      strides=1,\n",
    "                                      activation=tf.nn.relu)\n",
    "\n",
    "                L1 = tf.layers.max_pooling2d(inputs= L1,\n",
    "                                             pool_size=[2,2],\n",
    "                                             strides = 2,\n",
    "                                             padding = \"SAME\")\n",
    "\n",
    "                L2 = tf.layers.conv2d(inputs=L1,\n",
    "                                      filters = 64,\n",
    "                                      kernel_size=[3,3],\n",
    "                                      padding= \"SAME\",\n",
    "                                      strides=1,\n",
    "                                      activation=tf.nn.relu)\n",
    "\n",
    "                L2 = tf.layers.max_pooling2d(inputs= L2,\n",
    "                                             pool_size=[2,2],\n",
    "                                             strides = 2,\n",
    "                                             padding = \"SAME\")\n",
    "\n",
    "                L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "\n",
    "                W1 = tf.get_variable(\"weight1\",shape = [7*7*64,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b1 = tf.Variable(tf.random_normal([256]),name = \"bias1\")\n",
    "                _layer1 = tf.nn.relu(tf.matmul(L2,W1)+ b1)\n",
    "                layer1 = tf.layers.dropout (_layer1, rate = self.drop_rate)\n",
    "\n",
    "                W2 = tf.get_variable(\"weight2\",shape = [256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                b2 = tf.Variable(tf.random_normal([10]),name = \"bias2\")\n",
    "\n",
    "                self.logits = tf.matmul(layer1,W2)+ b2\n",
    "                self.H = tf.nn.relu(self.logits)\n",
    "\n",
    "                self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.Y))\n",
    "\n",
    "                self.train_net(self.mnist.images,self.mnist.labels)\n",
    "                self.get_accuracy(self.mnist.images,self.mnist.labels) \n",
    "                self.get_prediction(self.test_x_data)\n",
    "\n",
    "        # model 학습\n",
    "        def train_net(self, train_x_data, train_y_data):\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "            self.train = optimizer.minimize(self.cost)\n",
    "            \n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            training_epoch = 30\n",
    "            batch_size = 100 \n",
    "            print(\"학습\")\n",
    "            for step in range(training_epoch):\n",
    "                num_of_iter = int(self.mnist.num_examples / batch_size)\n",
    "                cost_val = 0\n",
    "                for i in range(num_of_iter):\n",
    "                    batch_x, batch_y = self.mnist.next_batch(batch_size)\n",
    "                    _,cost_val = sess.run([self.train, self.cost],\n",
    "                                         feed_dict = {self.X : batch_x, self.Y: batch_y, self.drop_rate:0.7})\n",
    "                print(cost_val)\n",
    "                    \n",
    "        # model의 Accuracy 측정\n",
    "        def get_accuracy(self,train_x_data, train_y_data):\n",
    "            predict = tf.argmax(self.H,1)\n",
    "            correct = tf.equal(predict,tf.argmax(self.Y,1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "            self.result = self.sess.run(self.accuracy , feed_dict = {self.X:train_x_data,\n",
    "                                                           self.Y: train_y_data,\n",
    "                                                           self.drop_rate:0.7})\n",
    "            print(\"정확도 : {}\".format(self.result))\n",
    "\n",
    "        # model의 prediction\n",
    "        def get_prediction(self,x_data):\n",
    "            self.sess.run(self.H,feed_dict={self.X:x_data,self.drop_rate:0.7})\n",
    "            \n",
    "## 1.Data loading\n",
    "mnist= input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "mnist= mnist.train\n",
    "test_x_data = pd.read_csv(\"./data/digitrecognizer/test.csv\")\n",
    "# test_x_data = pd.read_csv(\"./data/digitrecognizer/test.csv\")\n",
    "\n",
    "## 2. Model의 개수 지정 & 생성\n",
    "sess = tf.Session()\n",
    "num_of_model = 10\n",
    "cnn_models = [CnnModel(sess,\"Model\"+str(x),mnist,test_x_data) for x in range(num_of_model)]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
