## x와 y가 1차원 배열인 Ozone 예제

import tensorflow as tf
import numpy as np
import pandas as pd
import warnings
# 그래프 그려주는 python의 library
import matplotlib.pyplot as plt

warnings.filterwarnings(action = "ignore")
## 파일 불러오기 => pandas 이용하는 것이 가장 효율적
df = pd.read_csv("./data/ozone/ozone.csv",sep=",")
display(df)

## 데이터 정제/분류/생성 => Pandas에서 수행 + numpy

# 온도에 따른 오존량 예측
# 필요한 col 2개만 일단 추출 => fancy indexing

df2 = df[["Ozone","Temp"]]
# 결치값 제거
# dropna(how = "all") : Ozone과 Temp 행이 모두 NaN인 행 지우기
# dropna(how = "any") : Ozone과 Temp 행 중 하나라도 NaN이면 행 지우기

df3= df2.dropna(how = "any", inplace = False )
print(df2.shape)
print(df3.shape)

# 이렇게 준비한 데이터가 linear한 데이터인지 확인
# scatter : 데이터의 분포, 산전도 그려줌
plt.scatter(df3["Temp"],df3["Ozone"])
plt.show()

# placeholder
x = tf.placeholder(dtype = tf.float32)
y = tf.placeholder(dtype = tf.float32)

# training data set
x_data = df3["Temp"] # Series
y_data = df3["Ozone"]

# Weight & bias
W = tf.Variable(tf.random_normal([1]),name = "weight")
b = tf.Variable(tf.random_normal([1]),name = "bias")

# Hypothesis
H = W*x +b

# cost function
cost = tf.reduce_mean(tf.square(H-y))

# train node 생성
optimizer = tf.train.GradientDescentOptimizer(learning_rate =0.1)
train = optimizer.minimize(cost)

# 그래프 시
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습 (train)
for step in range(3000):
    _, cost_val = sess.run([train, cost],
                          feed_dict = {x:x_data,y:y_data})
    if step % 300 == 0:
        print("cost : {}".format(cost_val))
        
## 문제
# cost 값이 발산함 => 해 X
# x_data(Ozone)와 y_data(Temp)의 값의 차가 커서 error
# cost가 0으로 가까워져야 함!

## 해결
# 정규화를 통해 x_data와 y_data의 차이값을 0~1 사이의 값으로 변환

## 데이터 정제 : shrink

# 1. normalization : (요소값 - 최솟값) / (최댓값 - 최솟값)
#                 (요소값 - 최솟값) < (최댓값 - 최솟값)
#                 0 < (요소값 - 최솟값) / (최댓값 - 최솟값) < 1

# 2. standardization : (요소값 - 평균) / 표준편차

# placeholder
x = tf.placeholder(dtype = tf.float32)
y = tf.placeholder(dtype = tf.float32)

# training data set
## 데이터 정제 : 정규화
x_data = (df3["Temp"] -  df3["Temp"].min())/ (df3["Temp"].max()-df3["Temp"].min())
y_data = (df3["Ozone"] -  df3["Ozone"].min())/ (df3["Ozone"].max()-df3["Ozone"].min())

# Weight & bias
W = tf.Variable(tf.random_normal([1]),name = "weight")
b = tf.Variable(tf.random_normal([1]),name = "bias")

# Hypothesis
H = W*x +b

# cost function
cost = tf.reduce_mean(tf.square(H-y))

# train node 생성
optimizer = tf.train.GradientDescentOptimizer(learning_rate =0.1)
train = optimizer.minimize(cost)

# 그래프 실행 & 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습 (train)
for step in range(3000):
    _, cost_val = sess.run([train, cost],
                          feed_dict = {x:x_data,y:y_data})
    if step % 300 == 0:
        print("cost : {}".format(cost_val))
        
## 문제
# cost 값이 발산함 => 해 X
# x_data(Ozone)와 y_data(Temp)의 값의 차가 커서 error
# cost가 0으로 가까워져야 함!

## 해결
# 정규화를 통해 x_data와 y_data의 차이값을 0~1 사이의 값으로 변환

#--------------------------------------------------------------------------------
## x와 y가 다차원 배열인 Ozone 예제2
import tensorflow as tf
import numpy as np
import pandas as pd
import warnings
# pytho
# pip install sklearn
# 0~1 사이의 값으로 scaling해주는 library
from sklearn.preprocessing import MinMaxScaler
warnings.filterwarnings(action="ignore")

## Data loading
df = pd.read_csv("./data/ozone/ozone.csv",sep=",")
## 필요한 컬럼만 추출 : 2개의 열 지우기
df.drop(["Month","Day"],axis = 1, inplace= True)
## 결측값 제거
df.dropna(how ="any",inplace=True)

## x 데이터 추출 : dataframe type
df_x = df.drop("Ozone",axis = 1,inplace =False)
## y 데이터 추출 : series type
df_y = df["Ozone"]

# training data set : 데이터 정제
# values : 2차원 numpy array로 추출
#  MinMaxScaler().fit_transform() : 0~1 사이의 값으로 scaling
x_data = MinMaxScaler().fit_transform(df_x.values)
y_data = MinMaxScaler().fit_transform(df_y.values.reshape(-1,1))

# --------------------------------------------------------------
# tensorflow

# placeholder
X = tf.placeholder(shape=[None,3],dtype=tf.float32)
Y = tf.placeholder(shape=[None,1],dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random_normal([3,1]),name = "weight")
b = tf.Variable(tf.random_normal([1]),name = "bias")

# Hypothesis
# H = XW + b => 행렬 곱
H = tf.matmul(X,W)+b

# cost function
cost = tf.reduce_mean(tf.square(H - Y))

# 학습 노드 생성
train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)

# session & 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(30000):
    _,cost_val=sess.run([train,cost],
                           feed_dict={X:x_data,Y:y_data})
    if step % 30000 == 0:
        print(cost_val)
        
# prediction
# X : 2차원 배열의 1행 3열로 넘겨줘야 함
sess.run(H,feed_dict = {X:[[190,7.4,67]]})
