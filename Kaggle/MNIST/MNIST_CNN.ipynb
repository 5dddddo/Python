{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 module import\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29400, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. Data Loading\n",
    "mnist = pd.read_csv(\"./data/digitrecognizer/train.csv\")\n",
    "num_of_train = int(mnist.shape[0]*0.7)\n",
    "train_data = mnist.loc[:num_of_train,:]\n",
    "test_data = mnist.loc[num_of_train+1:,:]\n",
    "\n",
    "train_x_data = train_data.drop(\"label\",axis=1,inplace=False)\n",
    "train_y_data = tf.one_hot(train_data[\"label\"],10)\n",
    "sess = tf.Session()\n",
    "train_y_data = sess.run(train_y_data)\n",
    "\n",
    "test_x_data = test_data.drop(\"label\",axis=1,inplace=False)\n",
    "train_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-82dedc07543f>:33: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-82dedc07543f>:43: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "(?, 7, 7, 64)\n",
      "WARNING:tensorflow:From <ipython-input-4-82dedc07543f>:76: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "0.040531263\n",
      "0.0018923819\n",
      "Accuracy: 0.9931292533874512\n"
     ]
    }
   ],
   "source": [
    "## 2. Model 정의 (Tensorflow graph 생성)\n",
    "tf.reset_default_graph() # tensorflow graph 초기화\n",
    "## 2.1 Placeholder\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32)\n",
    "drop_rate = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "## 2.2 Convolution\n",
    "## CNN은 이미지 학습에 최적화 된 Deep learning 방법\n",
    "## 입력받은 이미지의 형태가 4차원 matrix\n",
    "## ( 이미지의 개수, 이미지의 width, 이미지의 height, color 수 )\n",
    "X_img = tf.reshape(X,[-1,28,28,1])\n",
    "\n",
    "## 2.3 Convolution Layer 1\n",
    "'''\n",
    "## filter의 shape = ( width, height, color 수, filter 수 )\n",
    "filter1 = tf.Variable(tf.random_normal([3,3,1,32]))\n",
    "## filter를 이용해서 Convolution image 생성\n",
    "L1 = tf.nn.conv2d(X_img,filter1,strides=[1,1,1,1], padding = \"SAME\")\n",
    "\n",
    "# 문제 : sigmoid는 0~1 사이의 실수로 값이 변화해서\n",
    "## 과정을 반복하다 보면 값이 0으로 수렴함\n",
    "\n",
    "## 해결 : 만들어진 Convolution에 relu 적용\n",
    "#L1 = tf.nn.relu(L1)\n",
    "'''\n",
    "# 위의 주석들이 아래 코드 한 줄로 정의 가능\n",
    "L1 = tf.layers.conv2d(inputs=X_img,\n",
    "                      filters = 32,\n",
    "                      kernel_size=[3,3],\n",
    "                      padding= \"SAME\",\n",
    "                      strides=1,\n",
    "                      activation=tf.nn.relu)\n",
    "\n",
    "## pooling 작업 (resize, sampling 작업) => Optional\n",
    "# L1 = tf.nn.max_pool(L1,ksize=[1,2,2,1], strides = [1,2,2,1],\n",
    "#                    padding = \"SAME\")\n",
    "\n",
    "# tf.layers.max_pooling2d() 로 변경\n",
    "L1 = tf.layers.max_pooling2d(inputs= L1,\n",
    "                             pool_size=[2,2],\n",
    "                             strides = 2,\n",
    "                             padding = \"SAME\")\n",
    "\n",
    "## 2.4 Convolution Layer 2\n",
    "L2 = tf.layers.conv2d(inputs=L1,\n",
    "                      filters = 64,\n",
    "                      kernel_size=[3,3],\n",
    "                      padding= \"SAME\",\n",
    "                      strides=1,\n",
    "                      activation=tf.nn.relu)\n",
    "\n",
    "L2 = tf.layers.max_pooling2d(inputs= L2,\n",
    "                             pool_size=[2,2],\n",
    "                             strides = 2,\n",
    "                             padding = \"SAME\")\n",
    "\n",
    "print(L2.shape) # (?, 7, 7, 64) => 7 x 7 크기의 img가 64개 있음\n",
    "\n",
    "## convolution 결과 : 28 x 28 크기의 원본 img가 \n",
    "## 7 x 7 크기의 img 64개로 변환\n",
    "\n",
    "## 2.5 Neural Network\n",
    "\n",
    "## Convolution의 결과(4차원)를\n",
    "## Neural network의 입력(2차원)으로\n",
    "## 사용하기 위해 shape 변경\n",
    "L2 = tf.reshape(L2, [-1,7*7*64])\n",
    "\n",
    "# 1번째 layer\n",
    "# logistic 256개\n",
    "W1 = tf.get_variable(\"weight1\",shape = [7*7*64,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([256]),name = \"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W1)+ b1)\n",
    "layer1 = tf.layers.dropout (_layer1, rate = drop_rate)\n",
    "\n",
    "# 2번째 layer\n",
    "W2 = tf.get_variable(\"weight2\",shape = [256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b2 = tf.Variable(tf.random_normal([10]),name = \"bias2\")\n",
    "\n",
    "logits = tf.matmul(layer1,W2)+ b2\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "## cost 함수\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "## train \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session & 초기화 \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch & batch size\n",
    "training_epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "# training\n",
    "for step in range(training_epoch):\n",
    "    num_of_iteration = int(train_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    \n",
    "    for i in range(num_of_iteration):\n",
    "        batch_x, batch_y = train_x_data[i*batch_size:(i+1)*batch_size],train_y_data[i*batch_size:(i+1)*batch_size]\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_x, Y: batch_y, drop_rate: 1.0})\n",
    "\n",
    "    if step %5 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#predict check\n",
    "predict = tf.argmax(H,1)\n",
    "# result = sess.run(predict, feed_dict={X:test_x_data, drop_rate: 1.0})\n",
    "# df = pd.DataFrame({\n",
    "#     'ImageId': [i for i in range(1,28001)],\n",
    "#     'Label': result\n",
    "# })\n",
    "# df.to_csv('./data/digitrecognizer/submission.csv', index=False)\n",
    "\n",
    "correct = tf.equal(predict, tf.math.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"Accuracy: {}\".format(sess.run(accuracy, feed_dict = {X: train_x_data, Y: train_y_data, drop_rate: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-23cfe08ed31b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m print('Accuracy:', sess.run(accuracy,\n\u001b[1;32m----> 5\u001b[1;33m                             feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(H, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy,\n",
    "                            feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결국 우리 MNIST 예제는 입력한 이미지 1개에 대해 예측한 결과가\n",
    "# H의 값으로 도출\n",
    "# [0.5, 0.8, 0.99, 0.12, 0.34, ...] 총 10개\n",
    "\n",
    "## 앙상블은 이런 model이 여러개 있음\n",
    "## H1 => [0.5, 0.8, 0.99, 0.12, 0.34, ...] \n",
    "## H2 => [0.45, 0.38, 0.9, 0.1, 0.7, ...] \n",
    "## H3=> [0.11, 0.2 0.19, 0.91, 0.47, ...]\n",
    "## H3=> [0.3, 0.32 0.3, 0.7, 0.4, ...]\n",
    "\n",
    "## H의 행별로 다 더함\n",
    "## SUM => [1.36, 0.8, 1.3, ..]\n",
    "## 최종 prediction은 SUM한 결과값을 가지고 예측\n",
    "\n",
    "## 장점 : 정확도 2% 증가\n",
    "## 단점 : Model 갯수만큼 시간 증가 ex) H 5개 -> 시간 5배 걸림"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
