# logistic regression을 이용하여 AND 연산을 학습

import tensorflow as tf
# training data set
x_data = [[0,0],[0,1],[1,0],[1,1]]
y_data = [[0],[0],[0],[1]]

#  placeholder 
X = tf.placeholder(shape=[None,2], dtype = tf.float32)
Y = tf.placeholder(shape=[None,1], dtype = tf.float32)

# weight & bias
# W의 [행,열] 중 열 값 == 다음 layer의 입력 갯수
W = tf.Variable(tf.random_normal([2,1]), name = "weight")
b = tf.Variable(tf.random_normal([1]), name = "bias")

# hypothesis
logits = tf.matmul(X,W) +b
H =  tf.sigmoid(logits)

# cost function
cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels = Y))

# train node 생성
optimizer = tf.train.GradientDescentOptimizer(learning_rate= 0.01)
train = optimizer.minimize(cost)

# session & 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습
for step in range(3000):
    _, cost_val = sess.run([train,cost], feed_dict = {X:x_data , Y:y_data})
    if step % 300 == 0:
        print(cost_val)

# accuracy 측정
predict = tf.cast(H> 0.5 , dtype = tf.float32)
correct = tf.equal(predict, Y)
accuracy = tf.reduce_mean(tf.cast(correct,dtype = tf.float32))

print("정확도 : {}".format(sess.run(accuracy , feed_dict = {X:x_data,
                                         Y:y_data})))
